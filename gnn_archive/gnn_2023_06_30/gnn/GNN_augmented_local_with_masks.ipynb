{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-iD3w-5pR6H_",
    "outputId": "17230612-5f4f-40a8-b7b4-232eef6c119f"
   },
   "outputs": [],
   "source": [
    "# !pip install torch_geometric\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "SuPcEnbASIyU"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch_geometric\n",
    "import torch.nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.nn import Linear, LayerNorm, ReLU, Dropout\n",
    "from torch_geometric.nn import ChebConv, NNConv, DeepGCNLayer, GATConv, DenseGCNConv, GCNConv, GraphConv, MessagePassing\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "import torch_geometric.transforms as T\n",
    "\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import scipy.sparse as sp\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.12.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eyOI5T-TSK7z",
    "outputId": "33932a3d-790f-4293-88f5-0ad9aef71f77"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets_folder = '/content/drive/MyDrive/Colab Notebooks/ssn/projekt/datasets/elliptic_augmented_dataset/'\n",
    "datasets_folder = '../datasets/elliptic_augmented_dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_edges_into_train_and_test(edges_dataframe, node_features_with_class):\n",
    "    edges_amount = len(edges_dataframe)\n",
    "    step = int(edges_amount * 0.02)    \n",
    "\n",
    "    train_edges = [] # edges which have both nodes labeled.\n",
    "    test_edges = []  # edges which have both nodes unlabeled.\n",
    "    \n",
    "    # this may take a long time\n",
    "    for i, edge in enumerate(np.array(edges_dataframe)):\n",
    "        if i % step == 0:\n",
    "            print(f'{ round(i/ edges_amount * 100)}%')\n",
    "            \n",
    "        found_start_node = node_features_with_class[node_features_with_class[0] == edge[0]]\n",
    "        \n",
    "        #ignore edges for which a node does not exist (this happened once)\n",
    "        if len(found_start_node) == 0: continue\n",
    "            \n",
    "        found_end_node = node_features_with_class[node_features_with_class[0] == edge[1]]\n",
    "        if len(found_end_node) == 0: continue\n",
    "            \n",
    "        if found_start_node.values[0, -1] != 2 and found_end_node.values[0, -1] != 2:\n",
    "            train_edges.append(edge)\n",
    "            continue\n",
    "        \n",
    "        if found_start_node.values[0, -1] == 2 and found_end_node.values[0, -1] == 2:\n",
    "            test_edges.append(edge)\n",
    "            continue\n",
    "          \n",
    "        # if found_start_node.values[0, -1] == 2:\n",
    "        #   test_edges.append(edge)\n",
    "        #   continue\n",
    "        \n",
    "        # if found_end_node.values[0, -1] == 2:\n",
    "        #   test_edges.append(edge)\n",
    "        # else:\n",
    "        #   train_edges.append(edge)\n",
    "          \n",
    "    train_edges = pd.DataFrame(train_edges)\n",
    "    test_edges = pd.DataFrame(test_edges)\n",
    "    return train_edges, test_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will read from cached files if they exist. If you change the code, delete the cached files.\n",
    "def split_edges_into_train_and_test_IO(datasets_folder):\n",
    "    train_edges_filepath = datasets_folder + 'train_edges.csv'\n",
    "    test_edges_filepath = datasets_folder + 'test_edges.csv'\n",
    "    \n",
    "    train_edges = [] # edges which have both nodes labeled.\n",
    "    test_edges = [] # edges which one or both nodes are unlabeled.\n",
    "    if os.path.isfile(train_edges_filepath) and os.path.isfile(test_edges_filepath):\n",
    "        train_edges = pd.read_csv(train_edges_filepath, sep=',').drop(columns=['Unnamed: 0']).rename(columns = {'0': 0, '1':1})\n",
    "        test_edges = pd.read_csv(test_edges_filepath, sep=',').drop(columns=['Unnamed: 0']).rename(columns = {'0': 0, '1':1})\n",
    "        \n",
    "    else:\n",
    "        edges_filepath = datasets_folder + 'elliptic_txs_edgelist.csv'\n",
    "        edges_dataframe = pd.read_csv(edges_filepath, sep=',')\n",
    "        train_edges, test_edges = split_edges_into_train_and_test(edges_dataframe, node_features_dataframe_with_class_without_timestep)\n",
    "        \n",
    "        # save the train_edges and test_edges to file so that the expensive split does not have to run every time.\n",
    "        train_edges.to_csv(train_edges_filepath)\n",
    "        test_edges.to_csv(test_edges_filepath)\n",
    "    \n",
    "    return train_edges, test_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'node_features_dataframe_with_class_without_timestep' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_edges, test_edges \u001b[38;5;241m=\u001b[39m \u001b[43msplit_edges_into_train_and_test_IO\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdatasets_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(train_edges))\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(test_edges))\n",
      "Cell \u001b[0;32mIn[21], line 15\u001b[0m, in \u001b[0;36msplit_edges_into_train_and_test_IO\u001b[0;34m(datasets_folder)\u001b[0m\n\u001b[1;32m     13\u001b[0m edges_filepath \u001b[38;5;241m=\u001b[39m datasets_folder \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124melliptic_txs_edgelist.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     14\u001b[0m edges_dataframe \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(edges_filepath, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 15\u001b[0m train_edges, test_edges \u001b[38;5;241m=\u001b[39m split_edges_into_train_and_test(edges_dataframe, \u001b[43mnode_features_dataframe_with_class_without_timestep\u001b[49m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# save the train_edges and test_edges to file so that the expensive split does not have to run every time.\u001b[39;00m\n\u001b[1;32m     18\u001b[0m train_edges\u001b[38;5;241m.\u001b[39mto_csv(train_edges_filepath)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'node_features_dataframe_with_class_without_timestep' is not defined"
     ]
    }
   ],
   "source": [
    "train_edges, test_edges = split_edges_into_train_and_test_IO(datasets_folder)\n",
    "print(len(train_edges))\n",
    "print(len(test_edges))\n",
    "train_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "Vd3GB0YrSLMk",
    "outputId": "1fa36373-defc-445c-8fb4-70f6514c6ae0"
   },
   "outputs": [],
   "source": [
    "classes_filepath = datasets_folder + 'elliptic_txs_classes.csv'\n",
    "classes_dataframe = pd.read_csv(classes_filepath, sep=',')\n",
    "classes_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r2eFNMdvTM7_"
   },
   "outputs": [],
   "source": [
    "# initially 2 is licit, 1 is illicit, suspicious is also illicit, unknown is unknown\n",
    "# after remapping 0 is illicit, 1 is licit, 2 is unknown\n",
    "# rows with class 0 and 1 will be used for training\n",
    "# rows with class 2 will be used for prediction\n",
    "\n",
    "def remap_label(label):\n",
    "  if label == '1': return 0\n",
    "  if label == 'suspicious': return 0\n",
    "  if label == '2': return 1\n",
    "  return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "B-gBA0omSkGn",
    "outputId": "19604d87-7d4e-411b-cfa6-db8f72419158"
   },
   "outputs": [],
   "source": [
    "classes_dataframe_after_remap = classes_dataframe.copy()\n",
    "classes_dataframe_after_remap['class'] = classes_dataframe['class'].map(remap_label)\n",
    "classes_dataframe_after_remap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "HrXrOogCT8_k",
    "outputId": "d03f26db-cfa7-43b3-cf6a-c7d453eb19a1"
   },
   "outputs": [],
   "source": [
    "node_features_filepath = datasets_folder + 'elliptic_txs_features.csv'\n",
    "node_features_dataframe = pd.read_csv(node_features_filepath, sep=',', header=None)\n",
    "# node_features_dataframe\n",
    "node_features_dataframe_with_class = node_features_dataframe.copy()\n",
    "node_features_dataframe_with_class['class'] = classes_dataframe_after_remap['class']\n",
    "# node_features_dataframe_with_class\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "ZnFCZ_IASfNo",
    "outputId": "632f5d6e-b5e7-411a-ca86-e9409c1d957e"
   },
   "outputs": [],
   "source": [
    "# Splitting nodes into train and test\n",
    "train_node_features = node_features_dataframe_with_class_without_timestep[node_features_dataframe_with_class_without_timestep['class'] != 2]\n",
    "test_node_features = node_features_dataframe_with_class_without_timestep[node_features_dataframe_with_class_without_timestep['class'] == 2]\n",
    "# train_node_features\n",
    "train_node_features_without_class = train_node_features.drop(columns=['class'])\n",
    "test_node_features_without_class = test_node_features.drop(columns=['class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reindex_dict(node_features_without_class): \n",
    "    reindex_dictionary = dict()\n",
    "    for i, node_id in enumerate(node_features_without_class[0]):\n",
    "      reindex_dictionary[node_id] = i\n",
    "    return reindex_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # now we need to reindex the train nodes and train edges so that node id is equal to row id\n",
    "def reindex_edges(node_features_without_class, edges):\n",
    "    reindex_dictionary = create_reindex_dict(node_features_without_class)\n",
    "    edges_reindexed = edges.copy()\n",
    "    edges_reindexed[0] = edges[0].map(lambda node_id: reindex_dictionary[node_id])\n",
    "    edges_reindexed[1] = edges[1].map(lambda node_id: reindex_dictionary[node_id])\n",
    "    return edges_reindexed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3O3onUoY6Mp"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "id": "4N7dBXL3Y8Ru",
    "outputId": "53c90ee4-8f69-425c-cada-4903c2097fe1"
   },
   "outputs": [],
   "source": [
    "train_edges_reindexed = reindex_edges(train_node_features_without_class, train_edges)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# attempting the same for test part\n",
    "reindex_dictionary = create_reindex_dict(test_node_features_without_class)\n",
    "# test_edges\n",
    "test_edges[0].map(lambda node_id: reindex_dictionary[node_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_edges_reindexed = reindex_edges(test_node_features_without_class, test_edges)\n",
    "# now we have test nodes & test edges which can be used to test the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QRROwyGQXmBj",
    "outputId": "9424be4d-a0cf-4082-dd8e-b3f6680de047"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FE6qCHdiXniP",
    "outputId": "81e58dee-5f00-449d-b266-21d934353dce"
   },
   "outputs": [],
   "source": [
    "# # now crafting the input data structures to pytorch geometric Data object\n",
    "# edge_index = torch.tensor(np.array(train_edges_reindexed), dtype=torch.long)\n",
    "# edge_index\n",
    "\n",
    "# x = torch.tensor(np.array(train_node_features_without_class), dtype=torch.float)\n",
    "# train_classes = train_node_features['class']\n",
    "# y = torch.tensor(np.array(train_classes), dtype=torch.float)\n",
    "\n",
    "# data = Data(x=x, edge_index=edge_index.t().contiguous(), y=y)\n",
    "# data.validate(raise_on_error = True)\n",
    "# split = T.RandomNodeSplit(num_val=0.1, num_test=0)\n",
    "# data_with_masks = split(data)\n",
    "# # print(data_with_masks.num_features)\n",
    "# # data_with_masks\n",
    "# # data.y\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_data_object(train_edges_reindexed, train_node_features_without_class, train_classes):\n",
    "    edge_index = torch.tensor(np.array(train_edges_reindexed), dtype=torch.long)\n",
    "    x = torch.tensor(np.array(train_node_features_without_class), dtype=torch.float)\n",
    "    y = torch.tensor(np.array(train_classes), dtype=torch.float)\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous(), y=y)\n",
    "    data.validate(raise_on_error = True)\n",
    "    split = T.RandomNodeSplit(num_val=0.1, num_test=0)\n",
    "    data_with_masks = split(data)\n",
    "    return data_with_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_test_data_object(test_edges_reindexed, test_node_features_without_class):\n",
    "    edge_index = torch.tensor(np.array(test_edges_reindexed), dtype=torch.long)\n",
    "    x = torch.tensor(np.array(test_node_features_without_class), dtype=torch.float)\n",
    "    data = Data(x=x, edge_index=edge_index.t().contiguous())\n",
    "    data.validate(raise_on_error = True)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_with_masks = create_train_data_object(train_edges_reindexed, train_node_features_without_class, train_node_features['class'])\n",
    "data_with_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_object = create_test_data_object(test_edges_reindexed, test_node_features_without_class)\n",
    "test_data_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarily we craft the test data structure to pytorch geometric Data object\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.hid = 8\n",
    "        self.in_head = 8\n",
    "        self.out_head = 1\n",
    "        self.conv1 = GATConv(data_with_masks.num_features, self.hid, heads = self.in_head)\n",
    "        self.conv2 = GATConv(self.hid * self.in_head, 1, concat=False, heads = self.out_head)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = F.dropout(x, p=0.6, training = self.training)\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.elu(x)\n",
    "        x = F.dropout(x, p=0.6, training = self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return F.sigmoid(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "data_in_device = data_with_masks.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=1e-5)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min')\n",
    "criterion = torch.nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train()\n",
    "for epoch in range(170):\n",
    "    optimizer.zero_grad()\n",
    "    out = model(data_in_device.x, data_in_device.edge_index)\n",
    "    out = out.reshape((data.x.shape[0]))\n",
    "    loss = criterion(out[data.train_mask], data.y[data.train_mask])\n",
    "    auc = roc_auc_score(data.y.detach().cpu().numpy(), out.detach().cpu().numpy()) #[train_idx]\n",
    "\n",
    "    if epoch%5 == 0:\n",
    "        train_auc = roc_auc_score(data.y[data.train_mask].detach().cpu().numpy(), out[data.train_mask].detach().cpu().numpy()) \n",
    "        val_auc = roc_auc_score(data.y[data.val_mask].detach().cpu().numpy(), out[data.val_mask].detach().cpu().numpy())\n",
    "        print(\"epoch: {} - loss: {} - train_roc: {} - val_auc: {}\".format(epoch, loss.item(), train_auc, val_auc))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now testing the model on unlabeled dataset to see if proportions between predicted classes are as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pqpH1vKNZuKj"
   },
   "outputs": [],
   "source": [
    "data_in_device = test_data_object.to(device)\n",
    "output = model(data_in_device.x, data_in_device.edge_index)\n",
    "output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amount_of_all_test_samples = output.shape[0]\n",
    "amount_of_licit_predicted_transactions = output.count_nonzero().item()\n",
    "amount_of_illicit_predicted_transactions = amount_of_all_test_samples - amount_of_licit_predicted_transactions\n",
    "\n",
    "print(f'{amount_of_illicit_predicted_transactions=}')\n",
    "print(f'{amount_of_licit_predicted_transactions=}')\n",
    "\n",
    "\n",
    "print(f'percent of illicit transactions')\n",
    "x = ['illicit', 'licit']\n",
    "y = [amount_of_illicit_predicted_transactions, amount_of_licit_predicted_transactions]\n",
    "plt.bar(x, y)\n",
    "# plt.text(x=0, y= 0,s= 0)\n",
    "plt.text(x='illicit', y= amount_of_illicit_predicted_transactions * 1.1,s= amount_of_illicit_predicted_transactions)\n",
    "plt.text(x='licit', y= amount_of_licit_predicted_transactions * 1.1,s= amount_of_licit_predicted_transactions)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cmzIerPHZuO_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
